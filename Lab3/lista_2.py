# -*- coding: utf-8 -*-
"""Lista 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nac43QS9dMMwytncwQja94vnJ_6c3Gd2

#Import library
"""

# Commented out IPython magic to ensure Python compatibility.
from ast import increment_lineno
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# %matplotlib inline

"""#Przygotowanie danych"""

os.getcwd()

from google.colab import drive
drive.mount("/content/drive")

os.chdir("/content/drive/MyDrive/Colab Notebooks/datasets/")

"""#Zadanie 1"""

data_dsp_6 = pd.read_csv("DSP_6.csv")

pd.isnull(data_dsp_6).sum()

# lub (gorszy moim zdaniem)
# porownujemy ten wynik z total number of columns
# data_dsp_6.info()

"""#Wyswietlenie zbioru (dla podgladu)

"""

data_dsp_6

"""#Zadanie 2"""

plt.figure(figsize=(8, 6))
data_dsp_6['Family_members'] = data_dsp_6['SibSp'] + data_dsp_6['Parch'] + 1
data_dsp_6['Family_members'].hist(bins=20, color='skyblue', alpha=0.7)
plt.title('Liczba członków rodziny na pokładzie')
plt.xlabel('Liczba członków rodziny')
plt.ylabel('Liczba pasażerów')
#plt.xticks(range(0, max(data_dsp_6['Family_members'])+1, 1)) #zeby byl co jeden na osi x
plt.grid(False)
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data_dsp_6['Fare'], bins=50, kde=True, color='salmon', alpha=0.7)
plt.title('Rozkład opłat uiszczonej przez pasażerów')
plt.xlabel('Opłata')
plt.ylabel('Liczba pasażerów')
plt.show()

"""#Zadanie 3"""

data_to_train = pd.read_csv('DSP_6.csv')

# Przygotowanie danych
data_to_train.drop(columns=["Cabin"], inplace=True)
data_to_train.fillna(data_to_train.mean(numeric_only=True), inplace=True)
data_to_train.dropna(inplace=True)

# Sprawdzenie odstajacych wartosci przed usunieciem ich
plt.figure(figsize=(5,5))
sns.boxplot(x="Pclass", y="Age", data=data_train_bad, showmeans=True, color='salmon')
plt.title('Sprawdzenie odstajacych wartosci')
plt.xlabel('Klasa')
plt.ylabel('Wiek')
plt.show()

# Podzial na klasy poniewaz sa 3
data_train_class1 = data_to_train.query('Pclass == 1').reset_index(drop=True)
data_train_class2 = data_to_train.query('Pclass == 2').reset_index(drop=True)
data_train_class3 = data_to_train.query('Pclass == 3').reset_index(drop=True)

#print(data_train_class1)
#print(data_train_class2)
#print(data_train_class3)

# Usuniecie przypadkow odstajacych
feature = 'Age'

# Klasa 1
Q1 = data_train_class1[feature].quantile(0.25)
Q3 = data_train_class1[feature].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

data_train_class1_no_outliers = data_train_class1[(data_train_class1[feature] >= lower_bound) & (data_train_class1[feature] <= upper_bound)]

# Klasa 2
Q1 = data_train_class2[feature].quantile(0.25)
Q3 = data_train_class2[feature].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

data_train_class2_no_outliers = data_train_class2[(data_train_class2[feature] >= lower_bound) & (data_train_class2[feature] <= upper_bound)]

# Klasa 3
Q1 = data_train_class3[feature].quantile(0.25)
Q3 = data_train_class3[feature].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

data_train_class3_no_outliers = data_train_class3[(data_train_class3[feature] >= lower_bound) & (data_train_class3[feature] <= upper_bound)]


data_train_no_outliers = pd.concat([data_train_class1_no_outliers, data_train_class2_no_outliers, data_train_class3_no_outliers]).reset_index()

# Podzielenie na dwa zbiory
data_train_good = data_train_no_outliers
data_train_bad = data_train_no_outliers

# Sprawdzenie odstajacych wartosci po usunieciu
plt.figure(figsize=(5,5))
sns.boxplot(x="Pclass", y="Age", data=data_train_bad, showmeans=True, color='salmon')
plt.title('Sprawdzenie odstajacych wartosci (po usunieciu ich)')
plt.xlabel('Klasa')
plt.ylabel('Wiek')
plt.show()

# trenowanie modelu bez konwersji danych
X = data_train_bad.drop(["Survived"], axis=1)
y = data_train_bad["Survived"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=101)

logmodel = LogisticRegression(max_iter=500)
logmodel.fit(X_train, y_train)
predictions = logmodel.predict(X_test)
print(classification_report(y_test, predictions))

"""pokazal sie blad could not convert string to float: 'Taussig, Miss. Ruth' oznacza on ze sa tam dane tekstowe (string), z ktorymi algorytm sobie nie radzi wiec wywal blad konwersji ze nie moze stringu zmienic na zmiennoprzecinkowa liczbe"""

data_train_good.head()

# transformacji zmiennych
sex = pd.get_dummies(data_train_good["Sex"], drop_first=True, dtype="int")
embark = pd.get_dummies(data_train_good["Embarked"], drop_first=True, dtype="int")
data_train_good_2 = pd.concat([data_train_good, sex, embark], axis=1)
data_train_good_2.drop(["Sex", "Embarked", "Name", "Ticket", "PassengerId"], axis=1, inplace=True)

# trenowanie po konwersji
X = data_train_good_2.drop(["Survived"], axis=1)
y = data_train_good_2["Survived"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=101)

logmodel = LogisticRegression(max_iter=500)
logmodel.fit(X_train, y_train)
predictions = logmodel.predict(X_test)
print(classification_report(y_test, predictions))

# trenowanie po konwersji (kilka iteracji)
X = data_train_good_2.drop(["Survived"], axis=1)
y = data_train_good_2["Survived"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=101)

logmodel = LogisticRegression(max_iter=1001)
logmodel.fit(X_train, y_train)
predictions = logmodel.predict(X_test)
print(classification_report(y_test, predictions))

# trenowanie po konwersji (kilka iteracji)
X = data_train_good_2.drop(["Survived"], axis=1)
y = data_train_good_2["Survived"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=6969)

logmodel = LogisticRegression(max_iter=1001)
logmodel.fit(X_train, y_train)
predictions = logmodel.predict(X_test)
print(classification_report(y_test, predictions))

"""Po zmianie random_state z 101 na 6969, zmienily sie:
F1 score z 0.81 na 0.78 dla wartosci jeden
Precision 0.88 na 0.85 wartosc 0, 0.81 na 0.82 wartosc 1
recall 0.88 na 0.90 dla 0 i dla 1 0.81 na 0.74 dla 1
i kilka innych rzeczy. Czyli odpowiadajac na pytanie tak cos sie zmienia w modelu :)

#Zadanie 4
"""

data_dsp_2 = pd.read_csv("DSP_2.csv")

# sprawdzenie czy sa dane puste
data_dsp_2.isnull().any()

# podglad na dane
data_dsp_2.head()

sns.set_style("darkgrid")
sns.countplot(x="Sex", data=data_dsp_2)

# malo danych kobiet jest w tym zbioze, moze powodowac to bledy lub nieprecyzyjnosc dla kobiet

sns.set_style("darkgrid")
sns.countplot(x="HeartDisease", data=data_dsp_2)

# wyglada ok

sns.displot(data_dsp_2["Age"], kde=False, color="red", bins=8)

# model bedzie najbarzdiej precyzyjnmy dla ludzi w srednim wieku, poniewaz jest znaczna mniejsza ilosc danych o mlodych jak i starych ludziach w zbiorze

# Preparing
chestpaintype = pd.get_dummies(data_dsp_2["ChestPainType"], drop_first=True, dtype="int")
stslope = pd.get_dummies(data_dsp_2["ST_Slope"], drop_first=True, dtype="int")
exerciseangina = pd.get_dummies(data_dsp_2["ExerciseAngina"], drop_first=True, dtype="int")
sex = pd.get_dummies(data_dsp_2["Sex"], drop_first=True, dtype="int")
restingecg = pd.get_dummies(data_dsp_2["RestingECG"], drop_first=True, dtype="int")

data_to_train_dsp_2 = pd.concat([data_dsp_2, sex, chestpaintype, restingecg, exerciseangina, stslope], axis=1)
data_to_train_dsp_2.drop(["ChestPainType", "Sex", "ST_Slope", "ExerciseAngina", "RestingECG"], axis=1, inplace=True)

# traning model
X=data_to_train_dsp_2.drop(["HeartDisease"], axis=1)
y=data_to_train_dsp_2["HeartDisease"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=101)
logmodel = LogisticRegression(max_iter=1001)
logmodel.fit(X_train, y_train)
predictions = logmodel.predict(X_test)

print(classification_report(y_test, predictions))

# podsumowanie, tak model cechuje sie dobrymi parametrami (wysokie F1),
# poniewaz ma bardzo dobre dane (dla ludzi w srednim wieku)
# Jesli chodzi czy moze zostac wykorzystany w praktyce tutaj zalezy:
# jezeli byl by przypadek ze dla osob w srednim wieku jest to tylko potrzebne to tak,
# ale jak mamy patrzec na ogol to mysle ze wymagaloby dodania danych dla osob mlodych i starych